{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900b9278",
   "metadata": {},
   "source": [
    "This file is aim to illustrate the basic syntax of pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "765ba43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Flatten\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import L1Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb12630",
   "metadata": {},
   "source": [
    "Two useful function: dir() and help(). The first function can list all the functions in the library. The second one can introduce the functionality of a specific function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4187c1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function is_available in module torch.cuda:\n",
      "\n",
      "is_available() -> bool\n",
      "    Return a bool indicating if CUDA is currently available.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The relationship is torch -> cuda -> cuda.is_available\n",
    "torch.cuda.is_available()\n",
    "dir(torch)\n",
    "dir(torch.cuda)\n",
    "dir(torch.cuda.is_available)\n",
    "help(torch.cuda.is_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca68ab0",
   "metadata": {},
   "source": [
    "How pytorch loads data? There are two libraries:\n",
    "1. Dataset: provide access to data and its label\n",
    "2. Dataload: package the data and provide them for the following netural networks\n",
    "\n",
    "For example, we want to divide pictures into two parts: ants and bees, ants and bees are their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c647a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Define our own class of dataset\n",
    "class MyData(Dataset):\n",
    "    # iniliatize the dataset\n",
    "    def __init__(self, root_dir, label_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.path = os.path.join(root_dir, label_dir)\n",
    "        self.img_path = os.listdir(self.path)\n",
    "\n",
    "    # get image and its label\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_path[index]\n",
    "        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)\n",
    "        img = Image.open(img_item_path)\n",
    "        label = self.label_dir\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    # return the size of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "    \n",
    "root_dir = \"dataset/train\"\n",
    "ants_label_dir = \"ants\"\n",
    "bees_label_dir = \"bees\"\n",
    "ants_dataset = MyData(root_dir, ants_label_dir)\n",
    "bees_dataset = MyData(root_dir, bees_label_dir)\n",
    "\n",
    "ants_dataset.__len__()\n",
    "ants_img, ants_label = ants_dataset.__getitem__(0)\n",
    "#ants_img.show()\n",
    "#print(ants_label)\n",
    "\n",
    "bees_dataset.__len__()\n",
    "bees_img, bees_label = bees_dataset.__getitem__(0)\n",
    "#bees_img.show()\n",
    "#print(bees_label)\n",
    "\n",
    "# Union two dataset\n",
    "train_dataset = ants_dataset + bees_dataset\n",
    "print(len(train_dataset) == len(ants_dataset) + len(bees_dataset))\n",
    "# Use index to call __get_item__\n",
    "img, label = train_dataset[150] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549c0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally I want to declare a method to write label of image to a .txt file\n",
    "\n",
    "root_dir = \"dataset/train\"\n",
    "target_dir = \"ants\"\n",
    "# The output of img_path is a list with string like \"12345.jpg\"\n",
    "img_path = os.listdir(os.path.join(root_dir, target_dir))\n",
    "# split function is a function used to seperate string\n",
    "# In this case target_dir.split(\"_\") = [\"ants\"]\n",
    "label = target_dir.split(\"_\")[0]\n",
    "out_dir = \"ants_label\"\n",
    "for i in img_path:\n",
    "    # use split to seperate different .jpg file\n",
    "    file_name = i.split('.jpg')[0]\n",
    "# Open file and write or read, two parameters are file_path and mode(read or write)\n",
    "    with open(os.path.join(root_dir, out_dir, \"{}.txt\".format(file_name)), \"w\") as f:\n",
    "        f.write(label)\n",
    "\n",
    "target_dir = \"bees\"\n",
    "img_path = os.listdir(os.path.join(root_dir, target_dir))\n",
    "label = target_dir.split(\"_\")[0]\n",
    "out_dir = \"bees_label\"\n",
    "for i in img_path:\n",
    "    file_name = i.split(\".jpg\")[0]\n",
    "    with open(os.path.join(root_dir, out_dir, \"{}.txt\".format(file_name)), \"w\") as f:\n",
    "        f.write(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15f923",
   "metadata": {},
   "source": [
    "Tensorboard is a function used to demonstrate the interaction of one variable with another variable. For example, when we train AI, we want to show the realtionship between loss rate and training rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# first we initialize a writer, which is preserved in file folder \"logs\"\n",
    "writer = SummaryWriter(\"logs\")\n",
    "image_path = \"data/train/ants_image/0013035.jpg\"\n",
    "img = Image.open(image_path)\n",
    "img = np.array(img)\n",
    "print(type(img))\n",
    "\n",
    "# need three parameters: tag, img_tensor(torch.Tensor, numpy or string), global_step\n",
    "# since the shape of numpy array is HWC(Height, Width, Channel), so you need to add a parameter: dataformats\n",
    "writer.add_image(\"test\", img, 2, dataformats='HWC')\n",
    "\n",
    "# need three parameters: tag(title of the chart), scalar_value(y axis), global_step(x axis)\n",
    "# The running result is saved in the \"logs\" folder under the current folder\n",
    "# If you want to show the chart, you can run \"tensorboard --logdir=logs --port==\" in terminal, the default port is 6006\n",
    "for i in range(100):\n",
    "    writer.add_scalar(\"y = 2x\", 2 * i, i)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f03a6",
   "metadata": {},
   "source": [
    "Transform is a python file used to process image. Input an image, use tools in transformer to resize and process and get a result.\n",
    "Another important concept is tensor, which is in the form of high dimensional array, containing parameters needed for neutral network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad4d4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3137, 0.3137, 0.3137,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3176, 0.3176, 0.3176,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3216, 0.3216, 0.3216,  ..., 0.3137, 0.3098, 0.3020],\n",
      "         ...,\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.1725, 0.3725, 0.3529],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3294, 0.3529, 0.3294],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3098, 0.3059, 0.3294]],\n",
      "\n",
      "        [[0.5922, 0.5922, 0.5922,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.5961, 0.5961, 0.5961,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.6000, 0.6000, 0.6000,  ..., 0.5922, 0.5882, 0.5804],\n",
      "         ...,\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.3608, 0.6196, 0.6157],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.5765, 0.6275, 0.5961],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.6275, 0.6235, 0.6314]],\n",
      "\n",
      "        [[0.9137, 0.9137, 0.9137,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9176, 0.9176, 0.9176,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9216, 0.9216, 0.9216,  ..., 0.9137, 0.9098, 0.9020],\n",
      "         ...,\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.5529, 0.9216, 0.8941],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.8863, 1.0000, 0.9137],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.9490, 0.9804, 0.9137]]])\n"
     ]
    }
   ],
   "source": [
    "img_path = \"data/train/ants_image/0013035.jpg\"\n",
    "# return a variable whose type is PIL image\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# first you should initialize a transformer tool by using .ToTensor()\n",
    "# second pass image parameter\n",
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_img = tensor_trans(img)\n",
    "print(tensor_img)\n",
    "\n",
    "# return a variable whose type is numpy.ndarray\n",
    "cv_img = cv2.imread(img_path)\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "writer.add_image(\"transformer\", tensor_img, 1)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954d9ec",
   "metadata": {},
   "source": [
    "Here are some classes of Transformer:\n",
    "1. ToTensor is a class which can turn an image into a tensor\n",
    "2. Normalize is a class which can normalize a tensor by giving mean and std of n channels. For a picture, we often have RGB channels.\n",
    "3. Resize is a class which can crop an PIL image, which return a PIL image\n",
    "4. RandomCrop is a class which can crop the given image at a random location\n",
    "5. Compose is a class which can put a lot of transformer classes together. For example, the following code put \"RandomCrop\" and \"ToTensor\" together, so the given image is first cropped at a random location, then turn the cropped image into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b74f8783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=2559x1439 at 0x175E1B39050>\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"logs\")\n",
    "img = Image.open(\"images/desktop.png\")\n",
    "print(img)\n",
    "\n",
    "# ToTensor\n",
    "trans_totensor = transforms.ToTensor()\n",
    "# img_tensor is an object of class \"ToTensor\", which can turn an image into a tensor\n",
    "img_tensor = trans_totensor(img)\n",
    "writer.add_image(\"ToTensor\", img_tensor)\n",
    "\n",
    "# Normalize\n",
    "trans_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "img_norm = trans_norm(img_tensor)\n",
    "writer.add_image(\"Normalize\", img_norm)\n",
    "\n",
    "# Resize\n",
    "trans_resize = transforms.Resize((512, 512))\n",
    "img_resize = trans_resize(img)\n",
    "img_resize_tensor = trans_totensor(img_resize)\n",
    "writer.add_image(\"Resize\", img_resize_tensor)\n",
    "\n",
    "# RandomCrop\n",
    "trans_random = transforms.RandomCrop(512)\n",
    "trans_compose = transforms.Compose([trans_random, trans_totensor])\n",
    "for i in range(10):\n",
    "    img_crop = trans_compose(img)\n",
    "    writer.add_image(\"Random Crop\", img_crop, i)\n",
    "\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d180fda",
   "metadata": {},
   "source": [
    "We can use torchvision to download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9073b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root=\"./datasets\", train=True, download=True, transform=dataset_transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./datasets\", train=False, download=True, transform=dataset_transform)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"logs\")\n",
    "for i in range(10):\n",
    "    img, target = test_set[i]\n",
    "    writer.add_image(\"test_set\", img, i)\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d709d",
   "metadata": {},
   "source": [
    "DataLoader is a function used to load data. There are many parameters:\n",
    "1. \"dataset\" is the directory of the dataset\n",
    "2. \"batch_size\" is the number of data per batch to load\n",
    "3. \"shuffle\" makes data reshuffled at every epoch when it is set \"True\"\n",
    "4. \"num_workers\" is the number of subprocesses used to load data\n",
    "5. \"drop_last\" can drop the last incomplete batch when it is set \"True\" if the size of dataset is not divisible by batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc7a40e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataset = torchvision.datasets.CIFAR10(root=\"./datasets\", download=True, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True, num_workers=0, drop_last=False)\n",
    "# for every element in test loader, it is a tuple containing a list of images and a list of targets\n",
    "# the shape of images is [batch_size, 3(3 channels), size, size]\n",
    "# targets are a list of labels of these images\n",
    "writer = SummaryWriter(\"logs\")\n",
    "step = 0\n",
    "epoch = 0\n",
    "for data in test_loader:\n",
    "    imgs, targets = data\n",
    "    # add images in one step\n",
    "    writer.add_images(f\"Epoch: {epoch}\", imgs, step)\n",
    "    step = step + 1\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b49663",
   "metadata": {},
   "source": [
    "Neutral Network\n",
    "1. Define a class which inharit nn.Module class\n",
    "2. forward is a function which can perform convolution operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68794133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input + 1\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = Model()\n",
    "input = torch.tensor(1.0)\n",
    "output = model(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f6be5",
   "metadata": {},
   "source": [
    "1. Convolution operation often has an input image in the form of tensor and a convolution kernel, which is also in the form of tensor. Every time we do a multiplication, take a subtensor of input image which has the same size as kernel, multiply corresponding positions and get output. \n",
    "2. Stride is a variable which decide the step. The submatrix is extracted from left to right and from top to bottom, stride can decide the number of elements each time passed.\n",
    "3. If we set \"padding = 1\", for a two dimensional tensor, fill in one row and one column each at the top, bottom, left, and right.\n",
    "4. If \"in_channel = 2\", there are two layers of input tensor, if \"out_channel = 2\", there are two convolutional kernels and we get two layers of output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a861a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[10, 12, 12],\n",
      "          [18, 16, 16],\n",
      "          [13,  9,  3]]]])\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[1, 2, 0, 3, 1], \n",
    "                      [0, 1, 2, 3, 1],\n",
    "                      [1, 2, 1, 0, 0],\n",
    "                      [5, 2, 3, 1, 1],\n",
    "                      [2, 1, 0, 1, 1]])\n",
    "kernel = torch.tensor([[1, 2, 1],\n",
    "                       [0, 1, 0],\n",
    "                       [2, 1, 0]])\n",
    "# the input of convolution function should have four parameters: batch, channel, height, width\n",
    "# reshape function can reshape a tensor, the original input is a two dimensional tensor, the new input is a four dimensional tensor\n",
    "input = torch.reshape(input, (1, 1, 5, 5))\n",
    "kernel = torch.reshape(kernel, (1, 1, 3, 3))\n",
    "\n",
    "output = F.conv2d(input, kernel, stride=1)\n",
    "print(output)\n",
    "\n",
    "# use torchvision download a test dataset and define a neutral network\n",
    "dataset = torchvision.datasets.CIFAR10(\"./datasets\", download=True, train=False, transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = Model()\n",
    "writer = SummaryWriter(\"logs\")\n",
    "step = 0\n",
    "for data in dataloader:\n",
    "    # img.shape = torch.Size([64, 3, 32, 32])\n",
    "    imgs, targets = data\n",
    "    # output.shape = torch.Size([64, 6, 30, 30]), has 6 channels which cannot be displayed in tensorboard directly\n",
    "    # perform forward operation autonomously, call __call__ function\n",
    "    output = model(imgs)\n",
    "    # if the value is set as -1, then the value can be calculated autonomously\n",
    "    output = torch.reshape(output, (-1, 3, 30, 30))\n",
    "    writer.add_images(\"input\", imgs, step)\n",
    "    step = step + 1\n",
    "    writer.add_images(\"output\", output, step)\n",
    "\n",
    "writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ced310",
   "metadata": {},
   "source": [
    "Pooling Layer:\n",
    "1. Pooling layer also has a kernal, each time take the maximal element of the corresponding submatrix of input as output\n",
    "2. If the size of the remaining submatrix is smaller than the size of the kernel, take the maximum element in the submatrix instead of discarding the submatrix when \"ceil mode\" is True\n",
    "3. \"stride\" is the same as convolution layer but the default value is kernal size\n",
    "4. MaxPool is used to preserve key features in order to reduce network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2964312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[1, 2, 0, 3, 1],\n",
    "                      [0, 1, 2, 3, 1],\n",
    "                      [1, 2, 1, 0, 0],\n",
    "                      [5, 2, 3, 1, 1],\n",
    "                      [2, 1, 0, 1, 1]], dtype=torch.float32)\n",
    "input = torch.reshape(input, (-1, 1, 5, 5))\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root=\"./datasets\", download=True, transform=transforms.ToTensor(), train=False)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=64)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.maxpool1(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = Model()\n",
    "writer = SummaryWriter(\"logs\")\n",
    "step = 0\n",
    "\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    writer.add_images(\"input\", imgs, step)\n",
    "    # the number of channels remain after MaxPool, so we don't need to reshape output\n",
    "    output = model(imgs)\n",
    "    writer.add_images(\"output\", output, step)\n",
    "    step = step + 1\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa05ca60",
   "metadata": {},
   "source": [
    "Non-linear Activations are some nonlinear functions used to process data\n",
    "1. ReLU(x) = max(0, x) ReLU has a parameter \"inplace\", whose default value is false. If it is true, the input will be inplaced by output. If it is false, the input will not be inplaced and we can get an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[1, -0.5],\n",
    "                      [-1, 3]])\n",
    "input = torch.reshape(input, (-1, 1, 2, 2))\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root=\"./datasets\", download=True, transform=transforms.ToTensor(), train=False)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.relu(input)\n",
    "        return output\n",
    "    \n",
    "writer = SummaryWriter(log_dir=\"logs\")\n",
    "model = Model()\n",
    "step = 0\n",
    "\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    writer.add_images(\"input\", imgs, step)\n",
    "    output = model(imgs)\n",
    "    writer.add_images(\"output\", output)\n",
    "    step = step + 1\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5006f4",
   "metadata": {},
   "source": [
    "Linear Layer: y = xA^T + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a8c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = torchvision.datasets.CIFAR10(download=True, root=\"./datasets\", transform=transforms.ToTensor(), train=False)\n",
    "dataloader = DataLoader(dataset, batch_size=64, drop_last=True)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # the first parameter is input feature, the second parameter is output feature\n",
    "        # which can be interpreted as dimension\n",
    "        self.linear = Linear(196608, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.linear(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = Model()\n",
    "\n",
    "for data in dataloader:\n",
    "    # the shape of images is [64, 3, 32, 32]\n",
    "    imgs, targets = data\n",
    "    # reshape the imgs to [[64*3*32*32]]\n",
    "    output = torch.flatten(imgs)\n",
    "    # the shape of output is [[10]]\n",
    "    output = model(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf405f",
   "metadata": {},
   "source": [
    "Sequential can put several network operations together such as convolution and linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c56092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "    \n",
    "model = Model()\n",
    "input = torch.ones((64, 3, 32, 32))\n",
    "output = model(input)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b554106",
   "metadata": {},
   "source": [
    "Loss function\n",
    "1. gradient is used to optimize weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "output = torch.tensor([1, 2, 5], dtype=torch.float32)\n",
    "\n",
    "input = torch.reshape(input, (1, 1, 1, 3))\n",
    "output = torch.reshape(output, (1, 1, 1, 3))\n",
    "\n",
    "loss = L1Loss(reduction=\"sum\")\n",
    "result = loss(input, output)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(download=True, train=False, root=\"./datasets\", transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model = Model()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        imgs, targets = data\n",
    "        outputs = model(imgs)\n",
    "        result_loss = loss(outputs, targets)\n",
    "        # set zero to remain independent\n",
    "        optim.zero_grad()\n",
    "        result_loss.backward()\n",
    "        optim.step()\n",
    "        running_loss = running_loss + result_loss\n",
    "    print(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e94c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    (Add_linear): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg16_false = torchvision.models.vgg16(pretrained=False)\n",
    "vgg16_true = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(root='./datasets', download=True, train=True, transform=transforms.ToTensor())\n",
    "\n",
    "# add to classifier\n",
    "vgg16_true.classifier.add_module(\"Add_linear\", Linear(1000, 10))\n",
    "print(vgg16_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03863b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg16 = torchvision.models.vgg16(pretrained=False)\n",
    "torch.save(vgg16, \"vgg16_method1.pth\")\n",
    "\n",
    "torch.save(vgg16.state_dict(), \"vgg16_method2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b30c2da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherlocked\\AppData\\Local\\Temp\\ipykernel_9984\\334303772.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"vgg16_method1.pth\")\n",
      "c:\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherlocked\\AppData\\Local\\Temp\\ipykernel_9984\\334303772.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vgg16.load_state_dict(torch.load(\"vgg16_method2.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"vgg16_method1.pth\")\n",
    "vgg16 = torchvision.models.vgg16(pretrained=False)\n",
    "vgg16.load_state_dict(torch.load(\"vgg16_method2.pth\"))\n",
    "print(model)\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc09c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The size of train dataset is 50000\n",
      "The size of test dataset is 10000\n",
      "Epoch: 1\n",
      "train step: 100, loss: 2.291564464569092\n",
      "train step: 200, loss: 2.284550189971924\n",
      "train step: 300, loss: 2.2580981254577637\n",
      "train step: 400, loss: 2.1759283542633057\n",
      "train step: 500, loss: 2.1407179832458496\n",
      "train step: 600, loss: 2.0684690475463867\n",
      "train step: 700, loss: 2.0098421573638916\n",
      "The total loss is: 1.9777332412719726, the total accuracy is: 0.3060999810695648\n",
      "Epoch: 2\n",
      "train step: 800, loss: 1.8626993894577026\n",
      "train step: 900, loss: 1.818153738975525\n",
      "train step: 1000, loss: 1.9473388195037842\n",
      "train step: 1100, loss: 1.9447749853134155\n",
      "train step: 1200, loss: 1.6946173906326294\n",
      "train step: 1300, loss: 1.6531866788864136\n",
      "train step: 1400, loss: 1.7546414136886597\n",
      "train step: 1500, loss: 1.8130407333374023\n",
      "The total loss is: 1.805677434539795, the total accuracy is: 0.3520999848842621\n",
      "Epoch: 3\n",
      "train step: 1600, loss: 1.7562133073806763\n",
      "train step: 1700, loss: 1.6662683486938477\n",
      "train step: 1800, loss: 1.9242268800735474\n",
      "train step: 1900, loss: 1.7216992378234863\n",
      "train step: 2000, loss: 1.8884921073913574\n",
      "train step: 2100, loss: 1.5305852890014648\n",
      "train step: 2200, loss: 1.5046805143356323\n",
      "train step: 2300, loss: 1.7704094648361206\n",
      "The total loss is: 1.702205696105957, the total accuracy is: 0.3814999759197235\n",
      "Epoch: 4\n",
      "train step: 2400, loss: 1.738261103630066\n",
      "train step: 2500, loss: 1.365604043006897\n",
      "train step: 2600, loss: 1.5743987560272217\n",
      "train step: 2700, loss: 1.6811949014663696\n",
      "train step: 2800, loss: 1.4714311361312866\n",
      "train step: 2900, loss: 1.6009811162948608\n",
      "train step: 3000, loss: 1.3138011693954468\n",
      "train step: 3100, loss: 1.5346300601959229\n",
      "The total loss is: 1.6755417015075684, the total accuracy is: 0.39659997820854187\n",
      "Epoch: 5\n",
      "train step: 3200, loss: 1.3633743524551392\n",
      "train step: 3300, loss: 1.485884189605713\n",
      "train step: 3400, loss: 1.4580838680267334\n",
      "train step: 3500, loss: 1.5174400806427002\n",
      "train step: 3600, loss: 1.6100735664367676\n",
      "train step: 3700, loss: 1.310607671737671\n",
      "train step: 3800, loss: 1.2724230289459229\n",
      "train step: 3900, loss: 1.4490835666656494\n",
      "The total loss is: 1.57945336227417, the total accuracy is: 0.4311999976634979\n",
      "Epoch: 6\n",
      "train step: 4000, loss: 1.4105976819992065\n",
      "train step: 4100, loss: 1.4402885437011719\n",
      "train step: 4200, loss: 1.522512435913086\n",
      "train step: 4300, loss: 1.206202745437622\n",
      "train step: 4400, loss: 1.1221833229064941\n",
      "train step: 4500, loss: 1.3900058269500732\n",
      "train step: 4600, loss: 1.4305890798568726\n",
      "The total loss is: 1.4908115699768067, the total accuracy is: 0.46480000019073486\n",
      "Epoch: 7\n",
      "train step: 4700, loss: 1.341406226158142\n",
      "train step: 4800, loss: 1.4780590534210205\n",
      "train step: 4900, loss: 1.3587226867675781\n",
      "train step: 5000, loss: 1.430619239807129\n",
      "train step: 5100, loss: 0.9961386322975159\n",
      "train step: 5200, loss: 1.2957180738449097\n",
      "train step: 5300, loss: 1.1916356086730957\n",
      "train step: 5400, loss: 1.3300338983535767\n",
      "The total loss is: 1.4138301536560058, the total accuracy is: 0.49559998512268066\n",
      "Epoch: 8\n",
      "train step: 5500, loss: 1.2105658054351807\n",
      "train step: 5600, loss: 1.2291195392608643\n",
      "train step: 5700, loss: 1.2073206901550293\n",
      "train step: 5800, loss: 1.2166879177093506\n",
      "train step: 5900, loss: 1.3248649835586548\n",
      "train step: 6000, loss: 1.536882996559143\n",
      "train step: 6100, loss: 1.0231212377548218\n",
      "train step: 6200, loss: 1.174843430519104\n",
      "The total loss is: 1.336297654724121, the total accuracy is: 0.5252000093460083\n",
      "Epoch: 9\n",
      "train step: 6300, loss: 1.3552589416503906\n",
      "train step: 6400, loss: 1.133768081665039\n",
      "train step: 6500, loss: 1.5313044786453247\n",
      "train step: 6600, loss: 1.07936692237854\n",
      "train step: 6700, loss: 1.0902202129364014\n",
      "train step: 6800, loss: 1.1858786344528198\n",
      "train step: 6900, loss: 1.1183934211730957\n",
      "train step: 7000, loss: 0.9479193687438965\n",
      "The total loss is: 1.2782618865966797, the total accuracy is: 0.5485000014305115\n",
      "Epoch: 10\n",
      "train step: 7100, loss: 1.3115965127944946\n",
      "train step: 7200, loss: 0.9704196453094482\n",
      "train step: 7300, loss: 1.097236156463623\n",
      "train step: 7400, loss: 0.8391241431236267\n",
      "train step: 7500, loss: 1.23993718624115\n",
      "train step: 7600, loss: 1.282970666885376\n",
      "train step: 7700, loss: 0.8450515270233154\n",
      "train step: 7800, loss: 1.2663848400115967\n",
      "The total loss is: 1.2382405975341797, the total accuracy is: 0.5633999705314636\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "train_data = torchvision.datasets.CIFAR10(root='./datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = torchvision.datasets.CIFAR10(root='./datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "\n",
    "print(f\"The size of train dataset is {train_data_size}\")\n",
    "print(f\"The size of test dataset is {test_data_size}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Construct neutral network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = Model()\n",
    "# use GPU\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# use GPU\n",
    "loss_fn = loss_fn.cuda()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "# Set parameters\n",
    "total_train_epoch = 0\n",
    "total_test_epoch = 0\n",
    "epoch = 10\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(f\"Epoch: {i+1}\")\n",
    "    # train\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        # use GPU\n",
    "        imgs = imgs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        output = model(imgs)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_epoch = total_train_epoch + 1\n",
    "        if (total_train_epoch % 100 == 0):\n",
    "            print(f\"train step: {total_train_epoch}, loss: {loss}\")\n",
    "\n",
    "    # guarantee no optimizing in order to test\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            imgs, targets = data\n",
    "            # use GPU\n",
    "            imgs = imgs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            output = model(imgs)\n",
    "            loss = loss_fn(output, targets)\n",
    "\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            accuracy = (output.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "    \n",
    "    total_test_loss = total_test_loss * 64 / test_data_size\n",
    "    total_accuracy = total_accuracy / test_data_size\n",
    "    total_test_epoch = total_test_epoch + 1\n",
    "    writer.add_scalar(\"test_loss\", total_test_loss, total_test_epoch)\n",
    "\n",
    "    print(f\"The total loss is: {total_test_loss}, the total accuracy is: {total_accuracy}\")\n",
    "\n",
    "    torch.save(model, f\"model_{i+1}.pth\")\n",
    "\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec601a",
   "metadata": {},
   "source": [
    "Here is the second method of using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6ad8779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The size of train dataset is 50000\n",
      "The size of test dataset is 10000\n",
      "Epoch: 1\n",
      "train step: 100, loss: 2.2944676876068115\n",
      "train step: 200, loss: 2.2946643829345703\n",
      "train step: 300, loss: 2.2825851440429688\n",
      "train step: 400, loss: 2.247307062149048\n",
      "train step: 500, loss: 2.2149949073791504\n",
      "train step: 600, loss: 2.129399061203003\n",
      "train step: 700, loss: 2.0366742610931396\n",
      "The total loss is: 2.0131363075256345, the total accuracy is: 0.2777000069618225\n",
      "Epoch: 2\n",
      "train step: 800, loss: 1.9144331216812134\n",
      "train step: 900, loss: 1.867685079574585\n",
      "train step: 1000, loss: 2.0080533027648926\n",
      "train step: 1100, loss: 1.9736117124557495\n",
      "train step: 1200, loss: 1.7371293306350708\n",
      "train step: 1300, loss: 1.654584527015686\n",
      "train step: 1400, loss: 1.7704800367355347\n",
      "train step: 1500, loss: 1.8128730058670044\n",
      "The total loss is: 1.932736734008789, the total accuracy is: 0.31459999084472656\n",
      "Epoch: 3\n",
      "train step: 1600, loss: 1.7423744201660156\n",
      "train step: 1700, loss: 1.6675219535827637\n",
      "train step: 1800, loss: 1.9287910461425781\n",
      "train step: 1900, loss: 1.708284854888916\n",
      "train step: 2000, loss: 1.9083821773529053\n",
      "train step: 2100, loss: 1.5239157676696777\n",
      "train step: 2200, loss: 1.4971745014190674\n",
      "train step: 2300, loss: 1.7972356081008911\n",
      "The total loss is: 1.7251334320068359, the total accuracy is: 0.37549999356269836\n",
      "Epoch: 4\n",
      "train step: 2400, loss: 1.7439993619918823\n",
      "train step: 2500, loss: 1.3420631885528564\n",
      "train step: 2600, loss: 1.5821107625961304\n",
      "train step: 2700, loss: 1.7073872089385986\n",
      "train step: 2800, loss: 1.472768783569336\n",
      "train step: 2900, loss: 1.5820852518081665\n",
      "train step: 3000, loss: 1.360253930091858\n",
      "train step: 3100, loss: 1.5650838613510132\n",
      "The total loss is: 1.7054541633605957, the total accuracy is: 0.3779999911785126\n",
      "Epoch: 5\n",
      "train step: 3200, loss: 1.3626916408538818\n",
      "train step: 3300, loss: 1.4712367057800293\n",
      "train step: 3400, loss: 1.4503108263015747\n",
      "train step: 3500, loss: 1.5491666793823242\n",
      "train step: 3600, loss: 1.5984289646148682\n",
      "train step: 3700, loss: 1.3521873950958252\n",
      "train step: 3800, loss: 1.2848526239395142\n",
      "train step: 3900, loss: 1.4676995277404785\n",
      "The total loss is: 1.6488830360412599, the total accuracy is: 0.4059999883174896\n",
      "Epoch: 6\n",
      "train step: 4000, loss: 1.355190396308899\n",
      "train step: 4100, loss: 1.4263182878494263\n",
      "train step: 4200, loss: 1.5617492198944092\n",
      "train step: 4300, loss: 1.2178490161895752\n",
      "train step: 4400, loss: 1.1185599565505981\n",
      "train step: 4500, loss: 1.3889763355255127\n",
      "train step: 4600, loss: 1.4147932529449463\n",
      "The total loss is: 1.5577966751098633, the total accuracy is: 0.4404999911785126\n",
      "Epoch: 7\n",
      "train step: 4700, loss: 1.3442360162734985\n",
      "train step: 4800, loss: 1.5260369777679443\n",
      "train step: 4900, loss: 1.3637795448303223\n",
      "train step: 5000, loss: 1.377601146697998\n",
      "train step: 5100, loss: 1.0078881978988647\n",
      "train step: 5200, loss: 1.307937502861023\n",
      "train step: 5300, loss: 1.1832737922668457\n",
      "train step: 5400, loss: 1.3900526762008667\n",
      "The total loss is: 1.4895319679260255, the total accuracy is: 0.46719998121261597\n",
      "Epoch: 8\n",
      "train step: 5500, loss: 1.1803748607635498\n",
      "train step: 5600, loss: 1.1822869777679443\n",
      "train step: 5700, loss: 1.1890398263931274\n",
      "train step: 5800, loss: 1.1983524560928345\n",
      "train step: 5900, loss: 1.3410186767578125\n",
      "train step: 6000, loss: 1.5600556135177612\n",
      "train step: 6100, loss: 1.0220998525619507\n",
      "train step: 6200, loss: 1.0735132694244385\n",
      "The total loss is: 1.4085410018920899, the total accuracy is: 0.5008000135421753\n",
      "Epoch: 9\n",
      "train step: 6300, loss: 1.4122700691223145\n",
      "train step: 6400, loss: 1.1058669090270996\n",
      "train step: 6500, loss: 1.547520399093628\n",
      "train step: 6600, loss: 1.09674870967865\n",
      "train step: 6700, loss: 1.0326884984970093\n",
      "train step: 6800, loss: 1.1456953287124634\n",
      "train step: 6900, loss: 1.0850012302398682\n",
      "train step: 7000, loss: 0.9072716236114502\n",
      "The total loss is: 1.3417437911987304, the total accuracy is: 0.5264999866485596\n",
      "Epoch: 10\n",
      "train step: 7100, loss: 1.2562412023544312\n",
      "train step: 7200, loss: 0.9189329743385315\n",
      "train step: 7300, loss: 1.1296334266662598\n",
      "train step: 7400, loss: 0.8526685237884521\n",
      "train step: 7500, loss: 1.1805002689361572\n",
      "train step: 7600, loss: 1.2953379154205322\n",
      "train step: 7700, loss: 0.8840444087982178\n",
      "train step: 7800, loss: 1.2825051546096802\n",
      "The total loss is: 1.2903069290161133, the total accuracy is: 0.5483999848365784\n"
     ]
    }
   ],
   "source": [
    "# define training device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Prepare dataset\n",
    "train_data = torchvision.datasets.CIFAR10(root='./datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = torchvision.datasets.CIFAR10(root='./datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "\n",
    "print(f\"The size of train dataset is {train_data_size}\")\n",
    "print(f\"The size of test dataset is {test_data_size}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Construct neutral network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "# Set parameters\n",
    "total_train_epoch = 0\n",
    "total_test_epoch = 0\n",
    "epoch = 10\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(f\"Epoch: {i+1}\")\n",
    "    # train\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        output = model(imgs)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_epoch = total_train_epoch + 1\n",
    "        if (total_train_epoch % 100 == 0):\n",
    "            print(f\"train step: {total_train_epoch}, loss: {loss}\")\n",
    "\n",
    "    # guarantee no optimizing in order to test\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            imgs, targets = data\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(imgs)\n",
    "            loss = loss_fn(output, targets)\n",
    "\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            accuracy = (output.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "    \n",
    "    total_test_loss = total_test_loss * 64 / test_data_size\n",
    "    total_accuracy = total_accuracy / test_data_size\n",
    "    total_test_epoch = total_test_epoch + 1\n",
    "    writer.add_scalar(\"test_loss\", total_test_loss, total_test_epoch)\n",
    "\n",
    "    print(f\"The total loss is: {total_test_loss}, the total accuracy is: {total_accuracy}\")\n",
    "\n",
    "    torch.save(model, f\"model_{i+1}.pth\")\n",
    "\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
